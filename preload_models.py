#!/usr/bin/env python3
"""
Script de pr√©-chargement des mod√®les pour optimiser les cold starts
Ex√©cut√© pendant le build du Docker pour t√©l√©charger et mettre en cache les mod√®les
"""

import os
import sys
import logging
from pathlib import Path

# Configuration du logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def preload_transformers_models():
    """Pr√©-charge les mod√®les Transformers"""
    try:
        from transformers import pipeline, AutoTokenizer, AutoModel
        
        logger.info("=== Pr√©-chargement des mod√®les Transformers ===")
        
        # Mod√®les utilis√©s dans votre application
        models_to_preload = [
            {
                "name": "astrosbd/french_emotion_camembert",
                "task": "text-classification",
                "description": "Sentiment analysis fran√ßais"
            },
            {
                "name": "joeddav/xlm-roberta-large-xnli", 
                "task": "zero-shot-classification",
                "description": "Classification zero-shot multilingue"
            }
        ]
        
        for model_info in models_to_preload:
            try:
                logger.info(f"T√©l√©chargement: {model_info['name']} ({model_info['description']})")
                
                # Pr√©-chargement du tokenizer et du mod√®le
                tokenizer = AutoTokenizer.from_pretrained(model_info["name"])
                model = AutoModel.from_pretrained(model_info["name"])
                
                # Test du pipeline pour s'assurer que tout fonctionne
                pipe = pipeline(model_info["task"], model=model_info["name"], device=-1)
                
                logger.info(f"‚úÖ {model_info['name']} pr√©-charg√© avec succ√®s")
                
            except Exception as e:
                logger.error(f"‚ùå Erreur lors du pr√©-chargement de {model_info['name']}: {e}")
                # Continue avec les autres mod√®les m√™me si un √©choue
                continue
                
    except ImportError as e:
        logger.error(f"Transformers non disponible: {e}")
        return False
    
    return True

def preload_sentence_transformers():
    """Pr√©-charge les mod√®les Sentence Transformers"""
    try:
        from sentence_transformers import SentenceTransformer
        
        logger.info("=== Pr√©-chargement Sentence Transformers ===")
        
        model_name = 'all-MiniLM-L6-v2'
        logger.info(f"T√©l√©chargement: {model_name}")
        
        # Pr√©-chargement et test
        model = SentenceTransformer(model_name)
        
        # Test rapide pour valider
        test_sentence = "Test de fonctionnement"
        embedding = model.encode(test_sentence)
        
        logger.info(f"‚úÖ {model_name} pr√©-charg√© avec succ√®s (embedding shape: {embedding.shape})")
        return True
        
    except ImportError as e:
        logger.error(f"Sentence Transformers non disponible: {e}")
        return False
    except Exception as e:
        logger.error(f"‚ùå Erreur lors du pr√©-chargement Sentence Transformers: {e}")
        return False

def preload_torch():
    """Pr√©-charge PyTorch et v√©rifie la configuration"""
    try:
        import torch
        
        logger.info("=== Configuration PyTorch ===")
        logger.info(f"PyTorch version: {torch.__version__}")
        logger.info(f"CUDA disponible: {torch.cuda.is_available()}")
        logger.info(f"CPU threads: {torch.get_num_threads()}")
        
        # Test de cr√©ation de tensor
        test_tensor = torch.randn(10, 10)
        logger.info("‚úÖ PyTorch configur√© correctement")
        
        return True
        
    except ImportError as e:
        logger.error(f"PyTorch non disponible: {e}")
        return False

def verify_model_cache():
    """V√©rifie que les mod√®les sont bien en cache"""
    cache_dir = os.environ.get('TRANSFORMERS_CACHE', '/app/model_cache')
    cache_path = Path(cache_dir)
    
    if cache_path.exists():
        cached_models = list(cache_path.rglob("*"))
        logger.info(f"üìÅ Cache mod√®les: {len(cached_models)} fichiers dans {cache_dir}")
        
        # Affiche les mod√®les en cache
        model_dirs = [d for d in cache_path.iterdir() if d.is_dir()]
        for model_dir in model_dirs[:5]:  # Limite √† 5 pour √©viter le spam
            logger.info(f"  - {model_dir.name}")
            
        return len(cached_models) > 0
    else:
        logger.warning(f"‚ùå R√©pertoire de cache non trouv√©: {cache_dir}")
        return False

def main():
    """Fonction principale de pr√©-chargement"""
    logger.info("üöÄ D√©but du pr√©-chargement des mod√®les ML")
    
    success_count = 0
    total_steps = 4
    
    # √âtapes de pr√©-chargement
    steps = [
        ("PyTorch", preload_torch),
        ("Transformers", preload_transformers_models), 
        ("Sentence Transformers", preload_sentence_transformers),
        ("V√©rification cache", verify_model_cache)
    ]
    
    for step_name, step_func in steps:
        logger.info(f"\n--- {step_name} ---")
        try:
            if step_func():
                success_count += 1
                logger.info(f"‚úÖ {step_name} termin√© avec succ√®s")
            else:
                logger.warning(f"‚ö†Ô∏è {step_name} termin√© avec des avertissements")
        except Exception as e:
            logger.error(f"‚ùå Erreur critique dans {step_name}: {e}")
    
    # R√©sum√©
    logger.info(f"\nüéØ Pr√©-chargement termin√©: {success_count}/{total_steps} √©tapes r√©ussies")
    
    if success_count >= 2:  # Au moins PyTorch + un mod√®le
        logger.info("‚úÖ Pr√©-chargement suffisant pour fonctionner")
        return 0
    else:
        logger.error("‚ùå Pr√©-chargement insuffisant - risque de probl√®mes au runtime")
        return 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
